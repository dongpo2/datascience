{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 自动化特征工程 - 特征生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "此文档是根据IBM 实验室在自动化特征工程领域的研究\"One button machine for automating feature engineering in relational databases\"及相关文献的思路对开放数据集“Berka dataset”进行特征工程的初步实践。\n",
    "</p>\n",
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "特征生成部分是根据实体间关系，通过属性相应规则进行特征转换，最后扁平化为一个初始建模数据集。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#--conf spark.sql.catalogImplementation=in-memory\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Feature Engineering Prepare example\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = os.environ['DSX_PROJECT_DIR']+'/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_data = os.environ['DSX_PROJECT_DIR']+'/../demo/datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. 读入数据文件形成数据关系表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 读入客户数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的客户文件 - client.csv，并构建Dataframe。\n",
    "<ul>\n",
    "    <li>client_id:&nbsp;&nbsp;客户id，主键</li>\n",
    "    <li>gender:&nbsp;&nbsp;性别 1-男&nbsp;&nbsp;2-女</li>\n",
    "    <li>birth_dt:&nbsp;&nbsp;生日，日期类型</li>\n",
    "    <li>district_id:&nbsp;&nbsp;分支机构id，分支区域外键</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_client = StructType([\n",
    "    StructField(\"client_id\", IntegerType(), True),\n",
    "    StructField(\"gender\", IntegerType(), True),\n",
    "    StructField(\"birth_dt\", DateType(), True),\n",
    "    StructField(\"district_id\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df_client = spark.read.csv(path_data+\"/client.csv\", header=True, schema=sch_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 读入账户数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的账户文件 - account.csv，并构建Dataframe。\n",
    "<ul>\n",
    "    <li>account_id:&nbsp;&nbsp;账户id，主键</li>\n",
    "    <li>district_id:&nbsp;&nbsp;账户所在地区，地区表外键</li>\n",
    "    <li>frequency:&nbsp;&nbsp;账单频率，1-每月&nbsp;&nbsp;2-每周&nbsp;&nbsp;3-交易发生时</li>\n",
    "    <li>open_id:&nbsp;&nbsp;开户日期</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_account = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"district_id\", IntegerType(), True),\n",
    "    StructField(\"frequency\", IntegerType(), True),\n",
    "    StructField(\"open_dt\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "df_account = spark.read.csv(path_data+\"/account.csv\", header=True, schema=sch_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+----------+\n",
      "|account_id|district_id|frequency|   open_dt|\n",
      "+----------+-----------+---------+----------+\n",
      "|       576|         55|        1|1993-01-01|\n",
      "|      3818|         74|        1|1993-01-01|\n",
      "+----------+-----------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_account.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3 读入交易数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的交易文件 - transaction.csv，并构建Dataframe。\n",
    "<ul>\n",
    "    <li>trans_id:&nbsp;&nbsp;交易id，主键</li>\n",
    "    <li>account_id:&nbsp;&nbsp;账户id，账户表外键</li>\n",
    "    <li>trans_dt:&nbsp;&nbsp;交易日期，日期类型</li>\n",
    "    <li>type:&nbsp;&nbsp;会计方式&nbsp;&nbsp;1-贷记(存入)&nbsp;&nbsp;2-借记(支取)</li>\n",
    "    <li>operation:&nbsp;&nbsp;操作类型&nbsp;&nbsp;1-信用卡取现&nbsp;&nbsp;2-现金存入&nbsp;&nbsp;3-从他行收款&nbsp;&nbsp;4-现金支取&nbsp;&nbsp;5-汇出他行</li>\n",
    "    <li>amount:&nbsp;&nbsp;交易金额</li>\n",
    "    <li>balance:&nbsp;&nbsp;账户余额</li>\n",
    "    <li>k_symbol:&nbsp;&nbsp;交易特点&nbsp;&nbsp;1-保险支付&nbsp;&nbsp;2-账单支付&nbsp;&nbsp;3-利息存入&nbsp;&nbsp;4-罚息&nbsp;&nbsp;5-家庭支出&nbsp;&nbsp;6-养老金支付&nbsp;&nbsp;7-贷款支付&nbsp;&nbsp;8-其它</li>\n",
    "    <li>bank:&nbsp;&nbsp;他行代码</li>\n",
    "    <li>account:&nbsp;&nbsp;他行账号</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_transaction = StructType([\n",
    "    StructField(\"trans_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"trans_dt\", DateType(), True),\n",
    "    StructField(\"type\", IntegerType(), True),\n",
    "    StructField(\"operation\", IntegerType(), True),\n",
    "    StructField(\"amount\", DecimalType(11,2), True),\n",
    "    StructField(\"balance\", DecimalType(11,2), True),\n",
    "    StructField(\"k_symbol\", IntegerType(), True),\n",
    "    StructField(\"bank\", StringType(), True),\n",
    "    StructField(\"account\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "df_transaction = spark.read.csv(path_data+\"/transaction.csv\", header=True, schema=sch_transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4 读入关系数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的关系文件 - disposition.csv，并构建Dataframe。关系文件主要用来描述客户、账户实体之间的多对多关系。\n",
    "<ul>\n",
    "    <li>disp_id:&nbsp;&nbsp;关系id，主键</li>\n",
    "    <li>client_id:&nbsp;&nbsp;客户id，客户表外键</li>\n",
    "    <li>account_id:&nbsp;&nbsp;账户id，账户表外键</li>\n",
    "    <li>type:&nbsp;&nbsp;关系类型&nbsp;&nbsp;1-拥有者&nbsp;&nbsp;2-使用者</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_disposition = StructType([\n",
    "    StructField(\"disp_id\", IntegerType(), True),\n",
    "    StructField(\"client_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"type\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "df_disposition = spark.read.csv(path_data+\"/disposition.csv\", header=True, schema=sch_disposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+----+\n",
      "|disp_id|client_id|account_id|type|\n",
      "+-------+---------+----------+----+\n",
      "|      1|        1|         1|   1|\n",
      "|      2|        2|         2|   1|\n",
      "+-------+---------+----------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_disposition.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.5 读入信用卡数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的信用卡数据文件 - card.csv，并构建Dataframe。信用卡是账户的一种服务。\n",
    "<ul>\n",
    "    <li>card_id:&nbsp;&nbsp;卡id，主键</li>\n",
    "    <li>disp_id:&nbsp;&nbsp;关系id，关系表外键</li>\n",
    "    <li>type:&nbsp;&nbsp;类型&nbsp;&nbsp;1-学生卡&nbsp;&nbsp;2-普通卡&nbsp;&nbsp;3-金卡</li>\n",
    "    <li>issued_dt:&nbsp;&nbsp;发卡日期，日期类型</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_card = StructType([\n",
    "    StructField(\"card_id\", IntegerType(), True),\n",
    "    StructField(\"disp_id\", IntegerType(), True),\n",
    "    StructField(\"type\", IntegerType(), True),\n",
    "    StructField(\"issued_dt\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "df_card = spark.read.csv(path_data+\"/card.csv\", header=True, schema=sch_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+----------+\n",
      "|card_id|disp_id|type| issued_dt|\n",
      "+-------+-------+----+----------+\n",
      "|   1005|   9285|   2|1993-11-07|\n",
      "|    104|    588|   2|1994-01-19|\n",
      "|    747|   4915|   2|1994-02-05|\n",
      "|     70|    439|   2|1994-02-08|\n",
      "|    577|   3687|   2|1994-02-15|\n",
      "+-------+-------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_card.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.6 读入贷款数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的贷款数据文件 - loan.csv，并构建Dataframe。贷款是账户的一种服务。一个账户理论可以有多笔贷款。在这个实验中，贷款表是主表。\n",
    "<ul>\n",
    "    <li>loan_id:&nbsp;&nbsp;贷款id，主键</li>\n",
    "    <li>account_id:&nbsp;&nbsp;账户id，账户表外键</li>\n",
    "    <li>granted_dt:&nbsp;&nbsp;审批日期，日期类型</li>\n",
    "    <li>amount:&nbsp;&nbsp;贷款金额</li>\n",
    "    <li>duration:&nbsp;&nbsp;贷款期限(月)</li>\n",
    "    <li>psyments:&nbsp;&nbsp;还款额</li>\n",
    "    <li>label:&nbsp;&nbsp;违约状态&nbsp;&nbsp;0-正常&nbsp;&nbsp;2-违约&nbsp;&nbsp;这个是目标变量</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_loan = StructType([\n",
    "    StructField(\"loan_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"granted_dt\", DateType(), True),\n",
    "    StructField(\"amount\", DecimalType(11,2), True),\n",
    "    StructField(\"duration\", IntegerType(), True),\n",
    "    StructField(\"payments\", DecimalType(11,2), True),\n",
    "    StructField(\"label\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df_loan = spark.read.csv(path_data+\"/loan.csv\", header=True, schema=sch_loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---------+--------+--------+-----+\n",
      "|loan_id|account_id|granted_dt|   amount|duration|payments|label|\n",
      "+-------+----------+----------+---------+--------+--------+-----+\n",
      "|   5314|      1787|1993-07-05| 96396.00|      12| 8033.00|    1|\n",
      "|   5316|      1801|1993-07-11|165960.00|      36| 4610.00|    0|\n",
      "+-------+----------+----------+---------+--------+--------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loan.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.7 读入他行支付数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的他行支付数据文件 - payorder.csv，并构建Dataframe。他行支付是该账户对他行的汇出指令。\n",
    "<ul>\n",
    "    <li>order_id:&nbsp;&nbsp;指令id，主键</li>\n",
    "    <li>account_id:&nbsp;&nbsp;账户id，账户表外键</li>\n",
    "    <li>bank_to:&nbsp;&nbsp;他行代码</li>\n",
    "    <li>account_to:&nbsp;&nbsp;他行账号</li>\n",
    "    <li>amount:&nbsp;&nbsp;支付金额</li>\n",
    "    <li>k_symbol:&nbsp;&nbsp;支付类型&nbsp;&nbsp;1-保险金支付&nbsp;&nbsp;2-家庭支付&nbsp;&nbsp;3-租借支付&nbsp;&nbsp;4-贷款支付&nbsp;&nbsp;6-其它</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_payorder = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"account_id\", IntegerType(), True),\n",
    "    StructField(\"bank_to\", StringType(), True),\n",
    "    StructField(\"account_to\", StringType(), True),\n",
    "    StructField(\"amount\", DecimalType(11,2), True),\n",
    "    StructField(\"k_symbol\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df_payorder = spark.read.csv(path_data+\"/payorder.csv\", header=True, schema=sch_payorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+----------+-------+--------+\n",
      "|order_id|account_id|bank_to|account_to| amount|k_symbol|\n",
      "+--------+----------+-------+----------+-------+--------+\n",
      "|   29401|         1|     YZ|  87144583|2452.00|       2|\n",
      "|   29402|         2|     ST|  89597016|3372.70|       4|\n",
      "+--------+----------+-------+----------+-------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_payorder.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.8 读入账户地区数据文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "读入经过预处理的账户地区数据文件 - district.csv，并构建Dataframe。\n",
    "<ul>\n",
    "    <li>district_id:&nbsp;&nbsp;地区id，主键</li>\n",
    "    <li>name:&nbsp;&nbsp;账户所在分支机构地区名称</li>\n",
    "    <li>region:&nbsp;&nbsp;地域</li>\n",
    "    <li>Inhabitants:&nbsp;&nbsp;地区人口</li>\n",
    "    <li>Municipalities499:&nbsp;&nbsp;该地区小于499居民的行政区数量</li>\n",
    "    <li>Municipalities1999:&nbsp;&nbsp;该地区在500-1999之间居民的行政区数量</li>\n",
    "    <li>Municipalities9999:&nbsp;&nbsp;该地区在2000-9999之间居民的行政区数量</li>\n",
    "    <li>Municipalities10000:&nbsp;&nbsp;该地区大于10000居民的行政区数量</li>\n",
    "    <li>cities:&nbsp;&nbsp;城市数</li>\n",
    "    <li>urbanratio:&nbsp;&nbsp;城市数</li>\n",
    "    <li>avgsalary:&nbsp;&nbsp;该地区平均工资</li>\n",
    "    <li>unemploy95:&nbsp;&nbsp;该地区1995年失业率</li>\n",
    "    <li>unemploy96:&nbsp;&nbsp;该地区1996年失业率</li>\n",
    "    <li>Enterpreneurs:&nbsp;&nbsp;该地区1000居民中企业家数量</li>\n",
    "    <li>crimes95:&nbsp;&nbsp;该地区1995年犯罪率</li>\n",
    "    <li>crimes96:&nbsp;&nbsp;该地区1996年犯罪率</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sch_district = StructType([\n",
    "    StructField(\"district_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"Inhabitants\", IntegerType(), True),\n",
    "    StructField(\"Municipalities499\", IntegerType(), True),\n",
    "    StructField(\"Municipalities1999\", IntegerType(), True),\n",
    "    StructField(\"Municipalities9999\", IntegerType(), True),\n",
    "    StructField(\"Municipalities10000\", IntegerType(), True),\n",
    "    StructField(\"cities\", IntegerType(), True),\n",
    "    StructField(\"urbanratio\", DecimalType(5,2), True),\n",
    "    StructField(\"avgsalary\", DecimalType(11,2), True),\n",
    "    StructField(\"unemploy95\", DecimalType(5,2), True),\n",
    "    StructField(\"unemploy96\", DecimalType(5,2), True),\n",
    "    StructField(\"Enterpreneurs\", IntegerType(), True),\n",
    "    StructField(\"crimes95\", IntegerType(), True),\n",
    "    StructField(\"crimes96\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df_district = spark.read.csv(path_data+\"/district.csv\", header=True, schema=sch_district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------------+-----------+-----------------+------------------+------------------+-------------------+------+----------+---------+----------+----------+-------------+--------+--------+\n",
      "|district_id|       name|         region|Inhabitants|Municipalities499|Municipalities1999|Municipalities9999|Municipalities10000|cities|urbanratio|avgsalary|unemploy95|unemploy96|Enterpreneurs|crimes95|crimes96|\n",
      "+-----------+-----------+---------------+-----------+-----------------+------------------+------------------+-------------------+------+----------+---------+----------+----------+-------------+--------+--------+\n",
      "|          1|Hl.m. Praha|         Prague|    1204953|                0|                 0|                 0|                  1|     1|    100.00| 12541.00|      0.29|      0.43|          167|   85677|   99107|\n",
      "|          2|    Benesov|central Bohemia|      88884|               80|                26|                 6|                  2|     5|     46.70|  8507.00|      1.67|      1.85|          132|    2159|    2674|\n",
      "+-----------+-----------+---------------+-----------+-----------------+------------------+------------------+-------------------+------+----------+---------+----------+----------+-------------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_district.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. 特征生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "根据梳理的路径，可以在技术上分为3层，从0层主表贷款表出发，到1层的账户表，再到2层的客户、地区、支付指令、交易、信用卡，最后是属性变量。具体计算上，采用从下向上的汇集方式。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_client.createOrReplaceTempView(\"client\")\n",
    "df_account.createOrReplaceTempView(\"account\")\n",
    "df_transaction.createOrReplaceTempView(\"transaction\")\n",
    "df_card.createOrReplaceTempView(\"card\")\n",
    "df_district.createOrReplaceTempView(\"district\")\n",
    "df_disposition.createOrReplaceTempView(\"disposition\")\n",
    "df_loan.createOrReplaceTempView(\"loan\")\n",
    "df_payorder.createOrReplaceTempView(\"payorder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1 生成客户到账户的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "客户表与账户表为多对多关系，可以从技术上拆分为一对多和多对一两个，首先处理账户到客户多对一的关系。\n",
    "</p>\n",
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "每个账户只有一个拥有着，因此，加上type=1条件可以形成多对一关系。而多对一关系在oneBM中成为一对一方式，转换时直接使用转换器即可，无需处理合计、平均等汇聚函数。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_client_ex = spark.sql(\"select d.account_id, d.client_id, c.gender, \\\n",
    "                datediff('1999-01-01', c.birth_dt) as birth_days from \\\n",
    "                client c, disposition d where c.client_id=d.client_id \\\n",
    "                and d.type=1 \\\n",
    "        \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----------+\n",
      "|account_id|client_id|gender|birth_days|\n",
      "+----------+---------+------+----------+\n",
      "|         1|        1|     2|     10246|\n",
      "|         2|        2|     1|     19689|\n",
      "+----------+---------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_client_ex.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 生成地区到账户的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "同样，地区表与账户表为一对多关系，在oneBM中成为一对一方式，转换时直接使用转换器即可。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "地区表中region字段实际是枚举类型，可以转换为数字类型，在oneBM中应该按照出现频率排序，此略。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|         region|\n",
      "+---------------+\n",
      "|         Prague|\n",
      "|  north Bohemia|\n",
      "|   east Bohemia|\n",
      "|  south Moravia|\n",
      "|  north Moravia|\n",
      "|central Bohemia|\n",
      "|   west Bohemia|\n",
      "|  south Bohemia|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct region from \\\n",
    "                district d, account a where a.district_id=d.district_id \\\n",
    "        \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_district_ex = spark.sql(\"select a.account_id, case region when 'Prague' then 1 \\\n",
    "            when 'north Bohemia' then 2 \\\n",
    "            when 'east Bohemia' then 3 \\\n",
    "            when 'south Moravia' then 4 \\\n",
    "            when 'north Moravia' then 5 \\\n",
    "            when 'central Bohemia' then 6 \\\n",
    "            when 'west Bohemia' then 7 \\\n",
    "            when 'south Bohemia' then 8 end as region_tp, \\\n",
    "            Inhabitants, Municipalities499, Municipalities1999, \\\n",
    "            Municipalities9999, Municipalities10000, cities, urbanratio, \\\n",
    "            avgsalary, unemploy95, unemploy96, Enterpreneurs, \\\n",
    "            crimes95, crimes96 \\\n",
    "            from district d, account a where a.district_id=d.district_id \\\n",
    "        \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 生成客户表的多对一方式特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "客户表与账户表还可以表达为多对一关系，即一个账户会有多个客户。因此，需要进行分组统计相应计算，由于客户表属性很少，统计计算只有一个计数。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_client_ex1 = spark.sql(\"select d.account_id, count(1) as client_cnt \\\n",
    "                from client c, disposition d where c.client_id=d.client_id \\\n",
    "                group by d.account_id \\\n",
    "        \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.4 生成支付指令表特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "支付指令表与账户表为多对一关系，即一个账户会有多个支付指令。因此，需要进行分组统计相应计算。\n",
    "</p>\n",
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "需要注意的是，由于每个账户的支付指令数量不一，从0到多，因此在分组计算中可能会出现null值，需要加以处理。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_payorder_ex = spark.sql(\"select a.account_id, \\\n",
    "            COALESCE(p.cnt, 0) as cnt, \\\n",
    "            COALESCE(p.amt_s, 0) as amt_s, \\\n",
    "            COALESCE(p.amt_a, 0) as amt_a, \\\n",
    "            COALESCE(p.amt_t, 0) as amt_t, \\\n",
    "            COALESCE(p.amt_x, 0) as amt_x, \\\n",
    "            COALESCE(p.symbol_1, 0) as symbol_1, \\\n",
    "            COALESCE(p.symbol_2, 0) as symbol_2, \\\n",
    "            COALESCE(p.symbol_3, 0) as symbol_3, \\\n",
    "            COALESCE(p.symbol_4, 0) as symbol_4, \\\n",
    "            COALESCE(p.symbol_6, 0) as symbol_6 \\\n",
    "            from account a left outer join \\\n",
    "            (select account_id, count(1) as cnt, \\\n",
    "            sum(amount) as amt_s, avg(amount) as amt_a, \\\n",
    "            case count(1) when 1 then 0 else std(amount) end as amt_t, max(amount) as amt_x, \\\n",
    "            sum(case k_symbol when 1 then 1 else 0 end) as symbol_1, \\\n",
    "            sum(case k_symbol when 2 then 1 else 0 end) as symbol_2, \\\n",
    "            sum(case k_symbol when 3 then 1 else 0 end) as symbol_3, \\\n",
    "            sum(case k_symbol when 4 then 1 else 0 end) as symbol_4, \\\n",
    "            sum(case k_symbol when 6 then 1 else 0 end) as symbol_6 \\\n",
    "            from payorder \\\n",
    "            group by account_id) p on a.account_id=p.account_id \\\n",
    "        \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.5 信用卡特征生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "信用卡表与账户表是多对一关系，由于计算最高卡类别方法与其它函数处理不同，因此分为两步。\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "以下是信用卡处理一般统计数量函数。需要注意的是并非所有账户都有信用卡，因此新卡数量特征需要相应赋值为0。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_card_ex1 = spark.sql(\"select a.account_id, COALESCE(c.cnt, 0) as card_cnt \\\n",
    "            from account a left outer join \\\n",
    "            (select d.account_id, count(1) as cnt \\\n",
    "            from card c, disposition d where c.disp_id=d.disp_id \\\n",
    "            group by d.account_id) c on a.account_id=c.account_id \\\n",
    "        \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "获取账户最高级别的信用卡相应属性。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_card_ex2 = spark.sql(\"select a.account_id, COALESCE(c.type, 0) as card_tp, \\\n",
    "            COALESCE(c.issued_days, -1) as card_issued_days \\\n",
    "            from account a left outer join \\\n",
    "            (select account_id, type, datediff('1999-01-01', issued_dt) as issued_days \\\n",
    "            from (select d.account_id, c.issued_dt, c.type, rank() \\\n",
    "                    over(partition by d.account_id, c.type order by c.issued_dt) as rk \\\n",
    "                    from (select dd.account_id, max(cc.type) as ty \\\n",
    "                            from card cc, disposition dd where cc.disp_id=dd.disp_id \\\n",
    "                            group by dd.account_id) a, card c, disposition d \\\n",
    "                    where c.disp_id=d.disp_id and a.account_id=d.account_id and c.type = a.ty) b \\\n",
    "            where rk=1) c on a.account_id=c.account_id \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.6 交易特征生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "交易表与账户表是多对一关系，需要进行分组处理。考虑到交易中还存在着操作类型等枚举字段，进一步可以进行基于枚举的分组统计。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_trans_ex = spark.sql(\"select account_id, count(1) as trans_cnt, \\\n",
    "                sum(amount) as amt_s, avg(amount) as amt_a, std(amount) as amt_d, \\\n",
    "                max(amount) as amt_x, min(amount) as amt_n, \\\n",
    "                sum(balance) as balance_s, avg(balance) as balance_a, std(balance) as balance_d, \\\n",
    "                max(balance) as balance_x, min(balance) as balance_n, \\\n",
    "                datediff('1999-01-01', max(trans_dt)) as trans_dt_r, \\\n",
    "                sum(case operation when 1 then 1 else 0 end) as symbol_1, \\\n",
    "                sum(case operation when 2 then 1 else 0 end) as symbol_2, \\\n",
    "                sum(case operation when 3 then 1 else 0 end) as symbol_3, \\\n",
    "                sum(case operation when 4 then 1 else 0 end) as symbol_4, \\\n",
    "                sum(case operation when 5 then 1 else 0 end) as symbol_5, \\\n",
    "                sum(case operation when 6 then 1 else 0 end) as symbol_6, \\\n",
    "                sum(case operation when 1 then amount else 0 end) as symbol_amt_1_s, \\\n",
    "                sum(case operation when 2 then amount else 0 end) as symbol_amt_2_s, \\\n",
    "                sum(case operation when 3 then amount else 0 end) as symbol_amt_3_s, \\\n",
    "                sum(case operation when 4 then amount else 0 end) as symbol_amt_4_s, \\\n",
    "                sum(case operation when 5 then amount else 0 end) as symbol_amt_5_s, \\\n",
    "                sum(case operation when 6 then amount else 0 end) as symbol_amt_6_s, \\\n",
    "                avg(case operation when 1 then amount else 0 end) as symbol_amt_1_a, \\\n",
    "                avg(case operation when 2 then amount else 0 end) as symbol_amt_2_a, \\\n",
    "                avg(case operation when 3 then amount else 0 end) as symbol_amt_3_a, \\\n",
    "                avg(case operation when 4 then amount else 0 end) as symbol_amt_4_a, \\\n",
    "                avg(case operation when 5 then amount else 0 end) as symbol_amt_5_a, \\\n",
    "                avg(case operation when 6 then amount else 0 end) as symbol_amt_6_a, \\\n",
    "                max(case operation when 1 then amount else 0 end) as symbol_amt_1_x, \\\n",
    "                max(case operation when 2 then amount else 0 end) as symbol_amt_2_x, \\\n",
    "                max(case operation when 3 then amount else 0 end) as symbol_amt_3_x, \\\n",
    "                max(case operation when 4 then amount else 0 end) as symbol_amt_4_x, \\\n",
    "                max(case operation when 5 then amount else 0 end) as symbol_amt_5_x, \\\n",
    "                max(case operation when 6 then amount else 0 end) as symbol_amt_6_x, \\\n",
    "                min(case operation when 1 then amount else 0 end) as symbol_amt_1_n, \\\n",
    "                min(case operation when 2 then amount else 0 end) as symbol_amt_2_n, \\\n",
    "                min(case operation when 3 then amount else 0 end) as symbol_amt_3_n, \\\n",
    "                min(case operation when 4 then amount else 0 end) as symbol_amt_4_n, \\\n",
    "                min(case operation when 5 then amount else 0 end) as symbol_amt_5_n, \\\n",
    "                min(case operation when 6 then amount else 0 end) as symbol_amt_6_n \\\n",
    "          from transaction group by account_id \\\n",
    "        \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.7 账户到贷款的特征生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_client_ex.createOrReplaceTempView(\"client_ex\")\n",
    "df_client_ex1.createOrReplaceTempView(\"client_ex1\")\n",
    "#df_account.createOrReplaceTempView(\"account\")\n",
    "df_trans_ex.createOrReplaceTempView(\"transaction_ex\")\n",
    "df_card_ex1.createOrReplaceTempView(\"card_ex1\")\n",
    "df_card_ex2.createOrReplaceTempView(\"card_ex2\")\n",
    "df_district_ex.createOrReplaceTempView(\"district_ex\")\n",
    "#df_loan_ex.createOrReplaceTempView(\"loan_ex\")\n",
    "df_payorder_ex.createOrReplaceTempView(\"payorder_ex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"text-indent: 2em; font-family: 微软雅黑; FONT-SIZE: 100%; COLOR: #191919; line-height: 1.5em;\">\n",
    "账户与贷款是一对多关系，因此在oneBM中属于一对一，可以直接使用特征。因此，只是简单连接多表。\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_account_ex = spark.sql(\"select a.account_id, a.frequency, datediff('1999-01-01', a.open_dt) as acc_open_days, \\\n",
    "            b.gender, b.birth_days, \\\n",
    "            c.client_cnt, \\\n",
    "            d.trans_cnt, d.amt_s, d.amt_a, d.amt_d, d.amt_x, d.amt_n, d.balance_s, d.balance_a, d.balance_d, \\\n",
    "            d.balance_x, d.balance_n, d.trans_dt_r, d.symbol_1, d.symbol_2, d.symbol_3, d.symbol_4, d.symbol_5, d.symbol_6, \\\n",
    "            d.symbol_amt_1_s, d.symbol_amt_2_s, d.symbol_amt_3_s, d.symbol_amt_4_s, d.symbol_amt_5_s, d.symbol_amt_6_s, \\\n",
    "            d.symbol_amt_1_a, d.symbol_amt_2_a, d.symbol_amt_3_a, d.symbol_amt_4_a, d.symbol_amt_5_a, d.symbol_amt_6_a, \\\n",
    "            d.symbol_amt_1_x, d.symbol_amt_2_x, d.symbol_amt_3_x, d.symbol_amt_4_x, d.symbol_amt_5_x, d.symbol_amt_6_x, \\\n",
    "            d.symbol_amt_1_n, d.symbol_amt_2_n, d.symbol_amt_3_n, d.symbol_amt_4_n, d.symbol_amt_5_n, d.symbol_amt_6_n, \\\n",
    "            e.card_cnt, \\\n",
    "            f.card_tp, f.card_issued_days, \\\n",
    "            g.region_tp, g.Inhabitants, g.Municipalities499, g.Municipalities1999, g.Municipalities9999, \\\n",
    "            g.Municipalities10000, g.cities, g.urbanratio, g.avgsalary, g.unemploy95, g.unemploy96, \\\n",
    "            g.Enterpreneurs, g.crimes95, g.crimes96, \\\n",
    "            h.cnt as payorder_cnt, h.amt_s as payorder_amt_s,  h.amt_a as payorder_amt_a, \\\n",
    "            h.amt_t as payorder_amt_t, h.amt_x as payorder_amt_x, h.symbol_1 as payorder_symbol_1, \\\n",
    "            h.symbol_2 as payorder_symbol_2, h.symbol_3 as payorder_symbol_3, h.symbol_4 as payorder_symbol_4, \\\n",
    "            h.symbol_6 as payorder_symbol_6 \\\n",
    "            from account a, client_ex b, client_ex1 c, \\\n",
    "            transaction_ex d, card_ex1 e, card_ex2 f, \\\n",
    "            district_ex g, payorder_ex h \\\n",
    "            where a.account_id=b.account_id \\\n",
    "            and a.account_id=c.account_id \\\n",
    "            and a.account_id=d.account_id \\\n",
    "            and a.account_id=e.account_id \\\n",
    "            and a.account_id=f.account_id \\\n",
    "            and a.account_id=g.account_id \\\n",
    "            and a.account_id=h.account_id \\\n",
    "            \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_account_ex.createOrReplaceTempView(\"account_ex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.8 处理贷款表本身属性，并输出为特征宽表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------+\n",
      "|  col_name|    data_type|comment|\n",
      "+----------+-------------+-------+\n",
      "|   loan_id|          int|   null|\n",
      "|account_id|          int|   null|\n",
      "|granted_dt|         date|   null|\n",
      "|    amount|decimal(11,2)|   null|\n",
      "|  duration|          int|   null|\n",
      "|  payments|decimal(11,2)|   null|\n",
      "|     label|          int|   null|\n",
      "+----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table loan\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_loan_ex = spark.sql(\"select a.*, \\\n",
    "            datediff('1999-01-01', l.granted_dt) as granted_days, \\\n",
    "            l.amount, l.duration, l.payments, l.label \\\n",
    "            from account_ex a, loan l where a.account_id=l.account_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_loan_ex_pd = df_loan_ex.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_loan_ex_pd.to_csv(path+\"/loan_ex2.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 作者\n",
    "\n",
    "**李英伟 liyingw@cn.ibm.com ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with DSX Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
